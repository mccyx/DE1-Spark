{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076905ba-5c2e-4a45-a9b5-44910d92e011",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# New API\n",
    "spark_session = SparkSession.builder\\\n",
    "        .master(\"spark://192.168.2.35:7077\") \\\n",
    "        .appName(\"Lecture1_Example4_wordcount_examples\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\", 2)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext\n",
    "spark_context.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75a94a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "book=spark_context.textFile(\"hdfs://192.168.2.35:9000/data/books/book-1.txt\")\n",
    "book.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8828778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd.map(): Return a new RDD by applying a function to each element of this RDD.\n",
    "## split each line into seperated words\n",
    "book_sp=book.map(lambda x: x.split(\" \"))\n",
    "book_sp.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87878221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd.filter(): Return a new RDD containing only the elements that satisfy a predicate.\n",
    "## for instance we can filter out sentences with too short phrases, which might be useless for analysis.\n",
    "book_sp_1=book_sp.filter(lambda x: len(x) > 1)\n",
    "book_sp_1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9948a52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd.flatMap(): Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results.\n",
    "## for example we can create single word RDD from previous result\n",
    "book_sw=book_sp_1.flatMap(lambda x: x)\n",
    "book_sw.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc73cfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd.groupBy(): Return an RDD of grouped items. Can be used to group the RDD elements by some condition.\n",
    "## for example we group the words by their length.\n",
    "book_sw_fl = book_sw.groupBy(lambda x: len(x))\n",
    "book_sw_fl.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359c81bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_sw_fl.mapValues(list).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0627ee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd.groupByKey(): Group the values for each key in the RDD into a single sequence, can be used to group RDD by key of elements.\n",
    "## NOTICE that the elements of RDD must be a (key,value) pair.\n",
    "## for example we can first construct (word,1) key-value pair, and then group by key, which is the word:\n",
    "book_sw_p = book_sw.map(lambda x: (x,1))\n",
    "book_wk=book_sw_p.groupByKey()\n",
    "book_wk.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcac1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .mapValues() to pass each value in the key-value pair through a map function\n",
    "book_wk.mapValues(list).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69a1da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd.reduceByKey(): Merge the values for each key using an associative and commutative reduce function.\n",
    "## NOTICE that the elements of RDD must be a (key,value) pair.\n",
    "## for example we can reduce the (word,1) key-value pair, and do wordcount:\n",
    "from operator import add\n",
    "book_wordcount = book_sw_p.reduceByKey(add)\n",
    "book_wordcount.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b21de0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hash seed to disable randomness\n",
    "import os\n",
    "os.environ[\"PYTHONHASHSEED\"]=str(123)\n",
    "# Frequency of the word \"Discovery\"\n",
    "book_wordcount.lookup(\"Discovery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d901637",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "book_wordcount.keys().take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a340cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd.distinct(): Return a new RDD containing the distinct elements in this RDD.\n",
    "# check the length of list before/after distinct\n",
    "print(\"Before .distinct():\",book_sw.count())\n",
    "print(\"After  .distinct():\",book_sw.distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28395e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd.keyBy(): Creates tuples of the elements in this RDD by applying f.\n",
    "## for example we can realize FirstLetterCount with this operation.\n",
    "book_sw.keyBy(lambda x: x[0]).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d653ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelined operation\n",
    "sorted(                                  # sort the results by alphabet\n",
    "    book.map(lambda x: x.split(\" \"))     # split each line into seperated words\n",
    "    .filter(lambda x: len(x) > 0)        # filter out empty lines\n",
    "    .flatMap(lambda x: x)                # flatMap to single words\n",
    "    .filter(lambda x: len(x) > 0)        # filter out empty words\n",
    "    .keyBy(lambda x: x[0].lower())       # extract the first letter and covert to lower case\n",
    "    .map(lambda x: (x[0],1))             # create (first_letter, 1) pairs\n",
    "    .reduceByKey(add)                    # reduce the key-value pair by adding up\n",
    "    .collect()                           # collect the result\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8c9c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function and use it in spark\n",
    "def key_pair(x):\n",
    "    return (x[0],x,1)\n",
    "book_sw.map(key_pair).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b6438b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
